---
title: "Developing the handles model"
output: html_document
---

```{r, echo=FALSE}
rm(list=ls())
cat('\f')

library(knitr)
library(ggplot2)
library(scales)
library(plyr)

print.table = function (df) {
  #kable(head(df))
  print(df)
}

print.data.table = function (dat) {
  dat = data.frame(dat)
  dat$name = NULL
  dat$time = NULL
  dat$has.issue = NULL
  print.table(dat)
}
```

## The Problem

I/O are currently detected using only the CPU data. We generally expect servers to be CPU bound, as node.js is very good at handling I/O concurrency. Thus if the CPU usage is low, that indicates that too much time is spent on waiting for I/O.

However, if by pressureing the server sufficiently hard with a lot of requests, it is almost always possible to get the CPU to 100%. The idea is to use the additional `handles` data as a second metric for detecting I/O issues.

## Getting the data

Matteo provided some sample files. To work on these in R, they should be converted to `.csv` files.

```
node node-clinic-doctor/debug/sample-to-csv.js small-load.clinic-doctor > small-load.csv

node node-clinic-doctor/debug/sample-to-csv.js heavy-load-1.clinic-doctor > heavy-load-1.csv
node node-clinic-doctor/debug/sample-to-csv.js heavy-load-2.clinic-doctor > heavy-load-2.csv

node node-clinic-doctor/debug/sample-to-csv.js mystery-1.clinic-doctor > mystery-1.csv
node node-clinic-doctor/debug/sample-to-csv.js mystery-2.clinic-doctor > mystery-2.csv
node node-clinic-doctor/debug/sample-to-csv.js mystery-3.clinic-doctor > mystery-3.csv
node node-clinic-doctor/debug/sample-to-csv.js mystery-4.clinic-doctor > mystery-4.csv

node node-clinic-doctor/debug/sample-to-csv.js flat-1.clinic-doctor > flat-1.csv
node node-clinic-doctor/debug/sample-to-csv.js flat-2.clinic-doctor > flat-2.csv
node node-clinic-doctor/debug/sample-to-csv.js flat-3.clinic-doctor > flat-3.csv
```

Unfortunately we currently don't have any real data, and the not `slow-io` servers have unrealistic constant handle trends. This makes it difficult to test our model against data without a handle issue. To work around this, the `small-load` data set is used as an example without an I/O issue. Even though it was specifically created to have an I/O issue, the I/O issue primarly shows up in the CPU data.

To load the data:

```{r}
load.data = function (name, has.issue) {
  dat = read.csv(name)
  dat$name = name
  dat$has.issue = has.issue
  dat$time = as.POSIXct((dat$timestamp - min(dat$timestamp)) / 1000, origin="1970-01-01", tz="GMT")
  return(dat)
};

dat.small.load = load.data('small-load.csv', F)
dat.heavy.load.1 = load.data('heavy-load-1.csv', T)
dat.heavy.load.2 = load.data('heavy-load-2.csv', T)
dat.mystery.1 = load.data('mystery-1.csv', F)
dat.mystery.2 = load.data('mystery-2.csv', F)
dat.mystery.3 = load.data('mystery-3.csv', T)
dat.mystery.4 = load.data('mystery-4.csv', T)
dat.flat.1 = load.data('flat-1.csv', F)
dat.flat.2 = load.data('flat-2.csv', F)
dat.flat.3 = load.data('flat-3.csv', F)
dat.increasing.1 = load.data('increasing-1.csv', T)
```

The data is structured as:

```{r, results='asis'}
print.data.table(head(dat.small.load))
```

The `interval` column has the value `1` for the gussed analysis interval. To focus only on this data:

```{r}
subset.interval = function (dat) {
  dat = dat[dat$interval == 1, ]
  dat$time = as.POSIXct((dat$timestamp - min(dat$timestamp)) / 1000, origin="1970-01-01", tz="GMT")
  return(dat)
}

dat.small.load = subset.interval(dat.small.load)
dat.heavy.load.1 = subset.interval(dat.heavy.load.1)
dat.heavy.load.2 = subset.interval(dat.heavy.load.2)
dat.mystery.1 = subset.interval(dat.mystery.1)
dat.mystery.2 = subset.interval(dat.mystery.2)
dat.mystery.3 = subset.interval(dat.mystery.3)
dat.mystery.4 = subset.interval(dat.mystery.4)
dat.flat.1 = subset.interval(dat.flat.1)
dat.flat.2 = subset.interval(dat.flat.2)
dat.flat.3 = subset.interval(dat.flat.3)
dat.increasing.1 = subset.interval(dat.increasing.1)
```

Reprinting the data, shows the `interval = 0` have now been stripped.

```{r, results='asis'}
print.data.table(head(dat.small.load))
```

Finally, the data is combined for convience:
```{r}
dat.main = rbind(dat.small.load, dat.heavy.load.1, dat.heavy.load.2)
dat.mystery = rbind(dat.mystery.1, dat.mystery.2, dat.mystery.3, dat.mystery.4)
dat.flat = rbind(dat.flat.1, dat.flat.2, dat.flat.3)
dat.increasing = rbind(dat.increasing.1)
```

## The model hypothesis

The model hypothesis is made by looking at the data and condering once domain knowledge. Particularly the latter should be the primary component when there isn't any real data.

```{r}
p = ggplot(dat.main, aes(x = time, y = handles, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
p = p + scale_y_continuous(limits = c(0, NA))
print(p)
```

The model hypothesis that Matteo provided was:

> servers with an I/O issue will have an increasing number of handles, that will occationally decrease a lot.

This appears to fit well with the `heavy-load` data and not very well with `small-load`. This is also what we want, as `small-load` is treated as having no issues with the `handles` data.

## Quantifying the model hypothesis

### Coefficient of variation

The immediate thought is that the `heavy-load` will have relatively more variance than `small-load` because it has more extream values. A challenge here is that the variance will depend on the number of concurrent requests. To normalize against this, the standard deviation is divided by the mean. This is what is called the coeffeicent of variation (`cv`).

```{r}
cv.unbiased = function (vec) {
  cv = sd(vec) / mean(vec)
  return(cv * (1 + 1/(4 * length(vec))))
}

analysis.cv = function (dat) {
  name = dat[1, 'name']
  return(data.frame(list(
    cv.unbiased = cv.unbiased(dat$handles)
  )))
}

print.table(ddply(dat.main, "name", analysis.cv))
```

The results quickly show that this is not the case. `heavy-load-1` does actally have a smaller coefficient of variation than `small-load`. 

### Heavy tail detection

Before going for the next idea it can be useful to at the data from other angels than just the plain graph. Once such way is the data distribution.

```{r, fig.height=4}
p = ggplot(dat.main, aes(handles, colour=has.issue))
p = p + geom_density(fill = 1, alpha = 0.1)
p = p + facet_wrap(~ name, scales='free')
print(p)
```

From this data, the idea is that that `heavy-load` data has more skewness (yes, this is a statistical term) than `small-load`. Once could then use a fancy statistical test like Jarque-Barre to test the skewness value. However, fancy statistical tests are a nightmare to implement in JavaScript. Instead the data is assumed to be normally distributed, from this assumtion one can check if there is a surprising amount of data at either distribution tails.

```{r, fig.height=3}
plot.heavy.tail = function (dat) {
  lower.bound = mean(dat$handles) - 1.96 * sd(dat$handles)
  upper.bound = mean(dat$handles) + 1.96 * sd(dat$handles)
  
  p = ggplot(dat, aes(x = time, y = handles, colour=has.issue))
  p = p + geom_line()
  p = p + scale_x_datetime(labels = date_format("%S sec"))
  p = p + geom_hline(yintercept = lower.bound)
  p = p + annotate("text", min(dat$time), lower.bound, vjust = -1, hjust=0, label = "lower bound")
  p = p + geom_hline(yintercept = upper.bound)
  p = p + annotate("text", max(dat$time), upper.bound, vjust = -1, hjust=1, label = "upper bound")
  print(p)
}

plot.heavy.tail(dat.small.load)
plot.heavy.tail(dat.heavy.load.1)
```

From the graphs, this appears to be a somewhat reasonable strategy. `small-load` is evenly distributed on both sides, `heavy-load-1` is is clearly not. This is then transformed into completly test function:

```{r}

analysis.heavy.tail = function (dat, risk = 0.05) {
  name = dat[1, 'name']
  has.issue = dat[1, 'has.issue']
  t.multipler = qt((1 - 0.05/2), nrow(dat))
  lower.bound = mean(dat$handles) - t.multipler * sd(dat$handles)
  upper.bound = mean(dat$handles) + t.multipler * sd(dat$handles)

    # extream tail ratio
  upper.extream.ratio = sum(dat$handles > upper.bound) / length(dat$handles)
  lower.extream.ratio = sum(dat$handles < lower.bound) / length(dat$handles)
  issue.deteted = max(upper.extream.ratio, lower.extream.ratio) > risk

  return(data.frame(list(
    lower = lower.extream.ratio,
    upper = upper.extream.ratio,
    detect.issue = issue.deteted,
    has.issue = has.issue
  )))
}

print.table(ddply(dat.main, "name", analysis.heavy.tail))
```

At this point life is good. Matteo then later showed data where this strategy doesn't work.

```{r}
print.table(ddply(dat.mystery, "name", analysis.heavy.tail))
```

As seen from this result, the issue detection (`detect.issue`) doesn't match the target (`has.issue`). Looking at the data, nothing appears to be unusual.

```{r}
p = ggplot(dat.mystery, aes(x = time, y = handles, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
p = p + scale_y_continuous(limits = c(0, NA))
print(p)
```

```{r, fig.height=4}
p = ggplot(dat.mystery, aes(handles, colour=has.issue))
p = p + geom_density(fill = 1, alpha = 0.1)
p = p + facet_wrap(~ name, scales='free')
print(p)
```

Looking at the results for `dat.mystery.4`, where an issue should be detected but isn't, nothing appears to be wrong. The detection logic just isn't good enogth. One could of course just and work around with small hacks, but this is rarely the solution.

```{r, fig.height=3}
plot.heavy.tail(dat.mystery.4)
```

What one should take from this, is that the model hypthoesis might be slightly wrong. The drops don't appear in `dat.mystery.4` are sudden but also a bit gradual.

## New model hypothesis

The previuse analysis, inspires a new hypothesis.

> servers with an I/O issue will have an increasing number of handles for a period followed by a period of decreasing number of handles.

### Sign change test

The immediate thought might be that one could fit a sinus function to this data. But without knowing what a good and bad period is, and because the period and amplitude may change over time, this is unlikely to be a good strategy.

Instead, increasing and decreasing trends are often best analysed by looking at the differential curves. This is where one looks at the difference from the previuse time step.

```{r}
diff.data = function (dat) {
  return(data.frame(list(
    timestamp = head(dat$timestamp, -1),
    time = head(dat$time, -1),
    handles.delta = diff(dat$handles),
    has.issue = head(dat$has.issue, -1)
  )))
}

dat.main.delta = ddply(dat.main, "name", diff.data)
dat.mystery.delta = ddply(dat.mystery, "name", diff.data)
```


```{r, fig.height=7}
p = ggplot(rbind(dat.main.delta, dat.mystery.delta), aes(x = time, y = handles.delta, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
print(p)
```

In the differential data transformation, the increasing part will be positive and the decreasing part will be negative. We can understand this better by looking at the just the signs.

```{r}
dat.main.delta$handles.delta.sign = sign(dat.main.delta$handles.delta)
dat.mystery.delta$handles.delta.sign = sign(dat.mystery.delta$handles.delta)
```

```{r, fig.height=7}
p = ggplot(rbind(dat.main.delta, dat.mystery.delta), aes(x = time, y = handles.delta.sign, colour=has.issue))
p = p + geom_point(alpha=0.1)
p = p + facet_grid(name ~ .)
p = p + scale_x_datetime(labels = date_format("%S sec"))
print(p)
```

From this it is apparent that the samples with an issue, has an uneven distribution of increasing and decreasing number of handles.

In classical statistics, a test that is often performed is the sign-test. This says, that if data is normally distributed then the next observation has a 50% change of having the opposite sign, compared to the previous observation. This test, does actually not just hold for normally distributed data but symetrically distributed data in general. Looking at the histrograms from before, and for the `dat.mystery` there do also appear to be a relation between symmetry and the number of handles. Although, the relation is not super apparent.

To get a better idea if this is true, a symmetry plot can be made:

```{r}
symmetry.data = function (dat) {
  handles.median = median(dat$handles.delta)
  handles.sorted = dat$handles.delta[order(dat$handles.delta)]
  
  return(data.frame(
    symmetry.x = rev(handles.sorted) - handles.median,
    symmetry.y = handles.median - handles.sorted,
    has.issue = dat$has.issue
  ))
}

dat.main.delta.symmetry = ddply(dat.main.delta, "name", symmetry.data)
dat.mystery.delta.symmetry = ddply(dat.mystery.delta, "name", symmetry.data)

p = ggplot(rbind(dat.main.delta.symmetry, dat.mystery.delta.symmetry), aes(symmetry.x, symmetry.y, colour=has.issue))
p = p + geom_point(alpha=0.3)
p = p + geom_abline(intercept = 0, slope = 1)
p = p + facet_wrap(~ name, scales='free')
print(p)
```

In this plot, data samples that isn't generally on the line can be said to be non-symetrical. From this plot the correlation between an issue and symmetry appears to be very strong.

The next step is then to implement the sign change test. To do this, we count the number of sign changes and compare that with the total number of observations. For symmetric data, the properbility of a sign change is 0.5. However this does not guarantee an exact split. To overcome this, the binomial distribution is used to tell the properbility of observing the given number of sign changes, given `n` observation and a properbility of 50%.

```{r}
analysis.sign.change.v1 = function (dat, risk = 0.001) {
  name = dat[1, 'name']
  has.issue = dat[1, 'has.issue']
  
  differential.data = diff(dat$handles)
  differential.data.sign = sign(differential.data)
  
  # count changes
  sign.changes = sum(diff(differential.data.sign) != 0)
  
  # lower tail p.value
  p.value = pbinom(sign.changes, nrow(dat), 0.5)
  
  return(data.frame(list(
    name = name,
    p.value = p.value,
    detect.issue = p.value < risk,
    has.issue = has.issue
  )))
}

print.table(ddply(rbind(dat.main, dat.mystery), "name", analysis.sign.change.v1))
```

Once again life is good. Matteo then later showed data where this doesn't work.

```{r}
print.table(ddply(dat.flat, "name", analysis.sign.change.v1))
```

### Improved sign change test

Luckily, the fix here is simple. The issue exists when the number of handles is almost constant: 

```{r}
p = ggplot(dat.flat, aes(x = time, y = handles, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
p = p + scale_y_continuous(limits = c(0, NA))
print(p)
```

This mostly happen in very theoretical cases, were there is very little or no I/O activity. A simple solution to this, is to only consider the observations were the number of observations actually changed. 

```{r}
analysis.sign.change.v1 = function (dat, risk = 0.001) {
  name = dat[1, 'name']
  has.issue = dat[1, 'has.issue']
  
  differential.data = diff(dat$handles)
  differential.data.sign = sign(differential.data)
  
  # count changes
  sign.changes = sum(diff(differential.data.sign) != 0)
  num.none.constant.obs = sum(differential.data.sign != 0)
  num.none.constant.obs = max(num.none.constant.obs, sign.changes)
  
  # lower tail p.value
  p.value = pbinom(sign.changes, num.none.constant.obs, 0.5)
  
  return(data.frame(list(
    name = name,
    p.value = p.value,
    detect.issue = p.value < risk,
    has.issue = has.issue,
    sign.changes = sign.changes,
    num.observations = num.none.constant.obs
  )))
}

print.table(ddply(rbind(dat.main, dat.mystery, dat.flat), "name", analysis.sign.change.v1))
```

## Increasing handles - A year after

After a year, a new type of issues, where the application is actually leaking handles, have shown up. Although intuitively the detection mecanism should be the same.

```{r}
p = ggplot(rbind(dat.flat, dat.increasing), aes(x = time, y = handles, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
p = p + scale_y_continuous(limits = c(0, NA))
print(p)
```

```{r}
print.table(ddply(rbind(dat.increasing), "name", analysis.sign.change.v1))
```


```{r}
dat.flat.delta = ddply(dat.flat, "name", diff.data)
dat.flat.delta$handles.delta.sign = sign(dat.flat.delta$handles.delta)

dat.increasing.delta = ddply(dat.increasing, "name", diff.data)
dat.increasing.delta$handles.delta.sign = sign(dat.increasing.delta$handles.delta)

p = ggplot(rbind(dat.flat.delta, dat.increasing.delta), aes(x = time, y = handles.delta.sign, colour=has.issue))
p = p + geom_point(alpha=0.1)
p = p + facet_grid(name ~ .)
p = p + scale_x_datetime(labels = date_format("%S sec"))
print(p)
```

The issue we see here, is that the `dat.increasing` dataset is not much different from the `dat.issue` issue dataset. The main difference is the initial increase, which signals that this is not a warmup but leaky file descriptors. While the `dat.issue` don't have an initial increase, but is more like a step function.

Finally, one should observe that the `dat.increasing` do have a heavy bias for an increasing sign (not surpring), thus it should be detected as an issue. This highlights a flaw in the `v1` appraoch, where observations that remains the same are not counted in `num.observations` but the sign change from $\pm 1$ to $0$ is counted. Which is contradicting, and problematic when there are losts of straight horisontal parts. A better appraoch is to simply filter out observations where nothing changed. This also makes sense from the sampling perspective, as no-change is also an artifict of the sampling procedure.

```{r}
sign.change.test.v2 = function (dat, change.p = 0.5) {
  vec = dat$handles
  
  differential.data = diff(vec)
  differential.data = differential.data[differential.data != 0]
  differential.data.sign = sign(differential.data)
  
  # count changes
  sign.changes = sum(diff(differential.data.sign) != 0)

  # lower tail p.value
  p.value = pbinom(sign.changes, length(differential.data.sign), change.p)
  
  return(data.frame(
    name = dat[1, 'name'],
    has.issue = dat[1, 'has.issue'],
    sign.changes=sign.changes,
    observations=length(differential.data.sign),
    p.value=p.value
  ))
}

analysis.sign.change.v2 = function (dat, risk = 1e-8) {
  name = dat[1, 'name']
  has.issue = dat[1, 'has.issue']
  
  p.value = sign.change.test.v2(dat)$p.value
  
  return(data.frame(list(
    name = name,
    p.value = p.value,
    detect.issue = p.value < risk,
    has.issue = has.issue
  )))
}

print.table(ddply(rbind(dat.main, dat.mystery, dat.flat, dat.increasing), "name", analysis.sign.change.v2))
```

The disadvantage here is that the `dat.issue` and `small-load`, have quite low `p.values`. The threshold thus have to be set, such that only very certain. While `dat.issue` might be acceptable as those are mostly theoretical edge-cases, the `small-load` is not great.

### skewness test

The random-walk assumtion in the sign-test only holds for non-increasing time series. Consider a linear curve, with added noise. It `diff` will then be a function of the random noise, without the linear signal.

```{r}
dat.increasing.synthetic = data.frame(
  timestamp=as.POSIXct(seq(0, 181), origin="1970-01-01", tz="GMT"),
  interval=1,
  delay=NA,
  cpu=NA,
  memory.rss=NA,
  memory.heapTotal=NA,
  memory.heapUsed=NA,
  memory.external=NA,
  handles=(c(seq(100, 1000, 10), rep(1000, 91)) + rnorm(182, 0, 10)),
  name='increasing-synthetic',
  has.issue=T,
  time=as.POSIXct(seq(0, 181), origin="1970-01-01", tz="GMT")
)
dat.increasing.synthetic.delta = ddply(dat.increasing.synthetic, "name", diff.data)

p = ggplot(dat.increasing.synthetic, aes(x = time, y = handles, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
p = p + scale_y_continuous(limits = c(0, NA))
print(p)
p = ggplot(dat.increasing.synthetic.delta, aes(x = time, y = handles.delta, colour=has.issue))
p = p + geom_line()
p = p + facet_grid(name ~ ., scales='free_y')
p = p + scale_x_datetime(labels = date_format("%S sec"))
print(p)
```

The result is that we can't detect an issue on such data.

```{r}
print.table(ddply(dat.increasing.synthetic, "name", analysis.sign.change.v2))
```

A likely solution is to apply a skewness test on the raw signal.

### Mira-skewness test

Chapter 5 in Advances in Directional and Linear Statistics, discusses various of non-parametic skewness tests. While they have their own suggestion, we will use the Mira-skewness test, which is a [Distribution-free test for symmetry based on Bonferroni's Measure](https://www.researchgate.net/publication/2783935_Distribution-free_test_for_symmetry_based_on_Bonferroni's_Measure).

Chapter 5 in Advances in Directional and Linear Statistics, shows that this is the best in terms of "type 1 error", while their own is best in terms of statistical power. In this case, "type 1 error" is the biggest concern. Thus we will use the Mira-skewness test.

```{r}
mira.skewness.test = function (dat) {
  vec = dat$handles
  
  sigma.2 = var(vec)
  mu = mean(vec)
  M = median(vec, type=3)
  n = length(vec)
  c = 0.5

  # compute skewness measurement
  bonferroni = 2 * (mu - M)
  
  # compute variance
  #S.median = mu - 2 * mean(vec * (vec <= M))
  S.median = mean(abs(vec - M))
  
  D.n.c = (n**(1/5)/(2*c)) * (
    quantile(vec, 1/2 + 0.5*n**(-1/5)      , type=3, names=F) -
    quantile(vec, 1/2 - 0.5*n**(-1/5) + 1/n, type=3, names=F)
  )
  S.c.2 = 4 * sigma.2 + D.n.c**2 - 4 * D.n.c * S.median

  # Compute Z statistics
  Z = sqrt(n) * bonferroni / sqrt(S.c.2)
  
  return(data.frame(
    name=dat[1, 'name'],
    has.issue = dat[1, 'has.issue'],
    p.value = 2 * (1 - pnorm(abs(Z))),
    bonferroni=bonferroni,
    Z=Z,
    S.c=sqrt(S.c.2)
  ))
}
print.table(ddply(rbind(dat.main, dat.mystery, dat.flat, dat.increasing, dat.increasing.synthetic), "name", mira.skewness.test))
```

Out of curiosity, here is the "Chapter 5 in Advances in Directional and Linear Statistics" approach.

```{r}
epanechnikov = function (vec) {
  return(ifelse(
    abs(vec) < sqrt(5),
    (3 / (4 * sqrt(5))) * (1 - (1/5) * vec**2),
    0
  ))
}

ghosh.skewness.test = function (dat) {
  vec = dat$handles
  vec.sorted = sort(vec)
  
  sigma.2 = var(vec)
  mu = mean(vec)
  M = median(vec, type=3)
  n = length(vec)
  
  # compute skewness measurement
  if (n %% 2 == 0) {
    indices.lower = seq(1, n/2)
    indices.upper = seq(n, n/2 + 1)
  } else {
    indices.lower = seq(1, (n+1)/2)
    indices.upper = seq(n, (n+1)/2)
  }
  upper.bounds = (vec.sorted[indices.lower] + vec.sorted[indices.upper]) / 2
  tau = (1/n) * sum(sapply(upper.bounds, function (upper.bound) mean(vec <= upper.bound)))
  
  # compute variance
  h.n = 1.06 * min(sqrt(sigma.2), IQR(vec, type=3) / 1.34) * n**(-1/5)
  f.mu = (1 / h.n) * mean(epanechnikov((mu - vec) / h.n))
  omega.2 = (1/16) - (1/4) * f.mu * mean(abs(vec - mu)) + (1/4)*(f.mu**2)*sigma.2
  
  # Compute Z statistics
  Z = sqrt(n) * ((tau - 0.25)/sqrt(omega.2))

  return(data.frame(list(
    name=dat[1, 'name'],
    has.issue = dat[1, 'has.issue'],
    p.value = 2 * (1 - pnorm(abs(Z))),
    tau=tau,
    Z=Z,
    omega=sqrt(omega.2)
  )))
}

print.table(ddply(rbind(dat.main, dat.mystery, dat.flat, dat.increasing, dat.increasing.synthetic), "name", ghosh.skewness.test))
```

For detecting an issue, either the sign-test or the mira-skewness-test should have a very low p-value.

```{r}
analysis.sign.change.v3 = function (dat, sign.risk = 1e-8, mira.risk = 1e-10) {
  name = dat[1, 'name']
  has.issue = dat[1, 'has.issue']

  sign.p.value = sign.change.test.v2(dat)$p.value
  mira.p.value = mira.skewness.test(dat)$p.value
  ghosh.p.value = ghosh.skewness.test(dat)$p.value
  
  return(data.frame(list(
    name = name,
    sign.p.value = sign.p.value,
    mira.p.value = mira.p.value,
    ghosh.p.value = ghosh.p.value,
    #detect.issue.v2 = sign.p.value < sign.risk,
    detect.issue.v3 = sign.p.value < sign.risk || sign.p.value < mira.risk,
    has.issue = has.issue
  )))
}

print.table(ddply(rbind(dat.main, dat.mystery, dat.flat, dat.increasing, dat.increasing.synthetic), "name", analysis.sign.change.v3))
```

The important result for mira, is that it should be high sensitive to increasing data, while not be sensitive to the almost flat data. This is a challenge as the `dat.flat` data also have an initial increase and from that perspective looks a lot like the `dat.increase` data. However, for this purpose `mira.skewness.test` appears to do a good job. 

## Conclusion

Using a non-paramatic `sign.test` on the difference data together with a non-paramatic skewness test on the raw data, seams to provide a reasonable classifier. The statistical tests do however demand high-certainty thresholds. This sugests that the tests do not measure the desired statistics, but rather an approximation or side-effect.

A more advanced model could be one based on baysian inference, which might provide more accuate classification properbilities.
